{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Udacity DEND Project-4: Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import configparser\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load config to envs and vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "\n",
    "#Normally this file should be in ~/.aws/credentials\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "#AWS_ACCESS_KEY_ID=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "#AWS_SECRET_ACCESS_KEY= config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "#INPUT_DATA_SD = config['AWS']['INPUT_DATA_SD']\n",
    "#INPUT_DATA_LD = config['AWS']['INPUT_DATA_LD']\n",
    "#OUTPUT_DATA = config['AWS']['OUTPUT_DATA']\n",
    "SONG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_SD_LOCAL']\n",
    "LOG_DATA_LOCAL=config['LOCAL']['INPUT_DATA_LD_LOCAL']\n",
    "OUTPUT_DATA_LOCAL=config['LOCAL']['OUTPUT_DATA_LOCAL']\n",
    "\n",
    "#print(AWS_ACCESS_KEY_ID)\n",
    "#print(AWS_SECRET_ACCESS_KEY)\n",
    "#print(INPUT_DATA)\n",
    "#print(OUTPUT_DATA)\n",
    "#print(SONG_DATA_LOCAL)\n",
    "#print(LOG_DATA_LOCAL)\n",
    "#print(OUTPUT_DATA_LOCAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark session with hadoop-aws package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load song_data (from JSON to Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "song_data_path = SONG_DATA_LOCAL\n",
    "\n",
    "# Use this instead if you want to read song_data from S3.\n",
    "#song_data_path = INPUT_DATA_SD\n",
    "\n",
    "df_sd = spark.read.json(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd.printSchema()\n",
    "df_sd.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (songs_table + artists_table) Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Songs table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd.createOrReplaceTempView(\"songs_table_DF\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "    SELECT song_id, title, artist_id, year, duration\n",
    "    FROM songs_table_DF\n",
    "    ORDER BY song_id\n",
    "\"\"\")\n",
    "songs_table.printSchema()\n",
    "songs_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = OUTPUT_DATA + \"songs_table\" + \"_\" + now\n",
    "print(songs_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF data to JSON file(s)\n",
    "# Ref: https://stackoverflow.com/questions/29908892/save-a-large-spark-dataframe-as-a-single-json-file-in-s3\n",
    "#df_sd.write.mode('append').json(songs_table_path)\n",
    "# -------\n",
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "# Partitioning: https://stackoverflow.com/questions/43731679/how-to-save-a-partitioned-parquet-file-in-spark-2-1\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songs_table_path = OUTPUT_DATA_LOCAL + \"songs_table.parquet\" + \"_\" + now\n",
    "\n",
    "# Use this instead if you want to store output to S3.\n",
    "#songs_table_path = OUTPUT_DATA + \"songs_table.parquet\" + \"_\" + now\n",
    "#print(songs_table_path)\n",
    "\n",
    "# NOTE: this command doesn't have partitioning!!\n",
    "#songs_table.write.parquet(songs_table_path)\n",
    "\n",
    "# Write DF to Spark parquet file (partitioned by year and artist_id)\n",
    "songs_table.write.partitionBy(\"year\", \"artist_id\").parquet(songs_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Artists table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd.createOrReplaceTempView(\"artists_table_DF\")\n",
    "artists_table = spark.sql(\"\"\"\n",
    "    SELECT  artist_id        AS artist_id, \n",
    "            artist_name      AS name, \n",
    "            artist_location  AS location, \n",
    "            artist_latitude  AS latitude, \n",
    "            artist_longitude AS longitude \n",
    "    FROM artists_table_DF\n",
    "    ORDER BY artist_id desc\n",
    "\"\"\")\n",
    "artists_table.printSchema()\n",
    "artists_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "artists_table_path = OUTPUT_DATA_LOCAL + \"artists_table.parquet\" + \"_\" + now\n",
    "\n",
    "# Use this instead if you want to store output to S3.\n",
    "#artists_table_path = OUTPUT_DATA + \"artists_table.parquet\" + \"_\" + now\n",
    "\n",
    "#print(artists_table_path)\n",
    "songs_table.write.parquet(artists_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load log_data (from JSON to Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read local song_data\n",
    "df_ld = spark.read.json(LOG_DATA_LOCAL)\n",
    "\n",
    "# Use this instead if you want to read log_data from S3.\n",
    "#df_ld = spark.read.json(INPUT_DATA_LD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld.printSchema()\n",
    "df_ld.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_filtered = df_ld.filter(df_ld.page == 'NextSong')\n",
    "df_ld_filtered.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create (users_table + time_table + songplays_table) Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Users table + write it to parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_filtered.createOrReplaceTempView(\"users_table_DF\")\n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT userId    AS user_id, \n",
    "                     firstName AS first_name, \n",
    "                     lastName  AS last_name, \n",
    "                     gender, \n",
    "                     level\n",
    "    FROM users_table_DF\n",
    "    ORDER BY last_name\n",
    "\"\"\")\n",
    "users_table.printSchema()\n",
    "users_table.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "users_table_path = OUTPUT_DATA_LOCAL + \"users_table.parquet\" + \"_\" + now\n",
    "print(users_table_path)\n",
    "users_table.write.parquet(users_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Time table + write it to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create timestamp column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with timestamp\n",
    "# \n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import types as t\n",
    "#from datetime import datetime\n",
    "\n",
    "@udf(t.TimestampType())\n",
    "def get_timestamp (ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0)\n",
    "    \n",
    "df_ld_filtered = df_ld_filtered.withColumn(\"timestamp\", get_timestamp(\"ts\"))\n",
    "                    \n",
    "\n",
    "df_ld_filtered.printSchema()\n",
    "df_ld_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datetime column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column with datetime\n",
    "# Ref: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dateformat#pyspark.sql.functions.from_unixtime\n",
    "@udf(t.StringType())\n",
    "def get_datetime(ts):\n",
    "    return datetime.fromtimestamp(ts / 1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "df_ld_filtered = df_ld_filtered.withColumn(\"datetime\", get_datetime(\"ts\"))\n",
    "df_ld_filtered.printSchema()\n",
    "df_ld_filtered.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://docs-snaplogic.atlassian.net/wiki/spaces/SD/pages/2458071/Date+Functions+and+Properties+Spark+SQL\n",
    "\n",
    "df_ld_filtered.createOrReplaceTempView(\"time_table_DF\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT  DISTINCT datetime AS start_time, \n",
    "                     hour(timestamp) AS hour, \n",
    "                     day(timestamp)  AS day, \n",
    "                     weekofyear(timestamp) AS week,\n",
    "                     month(timestamp) AS month,\n",
    "                     year(timestamp) AS year,\n",
    "                     dayofweek(timestamp) AS weekday\n",
    "    FROM time_table_DF\n",
    "    ORDER BY start_time\n",
    "\"\"\")\n",
    "time_table.printSchema()\n",
    "time_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "time_table_path = OUTPUT_DATA_LOCAL + \"time_table.parquet\" + \"_\" + now\n",
    "print(time_table_path)\n",
    "time_table.write.parquet(time_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create songplays table + write it to parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join song_data and log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://stackoverflow.com/questions/33745964/how-to-join-on-multiple-columns-in-pyspark\n",
    "df_ld_sd_joined = df_ld_filtered.join(df_sd, (df_ld_filtered.artist == df_sd.artist_name) & (df_ld_filtered.song == df_sd.title))\n",
    "df_ld_sd_joined.printSchema()\n",
    "df_ld_sd_joined.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ld_sd_joined = df_ld_sd_joined.withColumn(\"songplay_id\", f.monotonically_increasing_id())\n",
    "\n",
    "\n",
    "df_ld_sd_joined.createOrReplaceTempView(\"songplays_table_DF\")\n",
    "songplays_table = spark.sql(\"\"\"\n",
    "    SELECT  songplay_id AS songplay_id, \n",
    "            timestamp   AS start_time, \n",
    "            userId      AS user_id, \n",
    "            level       AS level,\n",
    "            song_id     AS song_id,\n",
    "            artist_id   AS artist_id,\n",
    "            sessionId   AS session_id,\n",
    "            location    AS location,\n",
    "            userAgent   AS user_agent\n",
    "    FROM songplays_table_DF\n",
    "    ORDER BY (user_id, session_id) \n",
    "\"\"\")\n",
    "\n",
    "songplays_table.printSchema()\n",
    "songplays_table.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DF to Spark parquet file\n",
    "# Ref: https://spark.apache.org/docs/latest/sql-data-sources-parquet.html\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H-%M-%S-%f')\n",
    "songplays_table_path = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\" + \"_\" + now\n",
    "print(songplays_table_path)\n",
    "time_table.write.parquet(songplays_table_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check files in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=config['AWS']['AWS_ACCESS_KEY_ID'],\n",
    "                       aws_secret_access_key=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "                     )\n",
    "song_data_path = \"udacity-dend\"\n",
    "log_data_path = INPUT_DATA + \"log_data/\"\n",
    "print(song_data_path)\n",
    "print(log_data_path)\n",
    "\n",
    "input_data_bucket =  s3.Bucket(song_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_sd = 0\n",
    "for obj in input_data_bucket.objects.filter(Prefix=\"song_data\"):\n",
    "    count_sd += 1\n",
    "    print(obj)\n",
    "print(count_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ld = 0\n",
    "for obj in input_data_bucket.objects.filter(Prefix=\"log_data\"):\n",
    "    count_ld += 1\n",
    "    print(obj)\n",
    "print(count_ld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from parque files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read songs_table\n",
    "input_data_parquet = OUTPUT_DATA_LOCAL + \"songplays_table.parquet\"\n",
    "df_sd = spark.read.parquet(input_data_parquet)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
